# âš”ï¸ KubeWorld âš”ï¸

![image alt](https://d33wubrfki0l68.cloudfront.net/69e55f968a6f44613384615c6a78b881bfe28bd6/42cd3/_common-resources/images/flower.svg)

## ğŸ“„ Sommaire ğŸ“„

- DÃ©coupage de l'infrastructure
- Mise en place des namespaces
- KubeBD
- Wordpress
- RBAC
- Monitoring
- OIDC
- Registry + GUI Web

## âœ‚ï¸ DÃ©coupage de l'infrastructure âœ‚ï¸

Le dÃ©coupage de l'infrastructure serait d'**un namespace par client**.
Ce choix est guidÃ© par la volontÃ© de garder les environements des clients indÃ©pendant des uns des autres.
Autrement dit, si un l'environnement du **client A** pose problÃ¨me, il ne pourra impacter le bon fonctionnement de l'environnement du **client B**.

### Composants

A chaque namespace sera affectÃ© un **quota**. Ce mÃªme namespace sera composÃ©:
- d'un deployment
    - Wordpress
- d'un ingress
- d'un service
- d'un KubeDB
- d'un RBAC

### Organisation des fichiers et des dossiers

Comme chaque client Ã  des besoins unique, les configurations des clients seront aussi sÃ©parÃ©es.

La structure correspondra Ã  ce modÃ¨le:

![](https://i.imgur.com/uXYKNiY.png)


## ğŸ· Mise en place des namespaces ğŸ·



```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: test
  labels:
    name: test
```



## ğŸ—ƒï¸ KubeBD ğŸ—ƒï¸

### Description

KubeDB est un framework pour Kubectl permetant de simplifier la "containerisation" des base de donnÃ©es.

Elle permet par exemple de:

- CrÃ©er une base de donnÃ©es dÃ©clarative Ã  l'aide de CRD.
- Effectuer des sauvegardes ponctuelles ou pÃ©riodiques dans diverse "cloud stores", par exemple S3, GCS, etc.
- Restaurer Ã  partir d'une sauvegarde ou cloner toute base de donnÃ©es.
- IntÃ©gration native avec Prometheus pour la surveillance via l'opÃ©rateur Prometheus de CoreOS.
- Appliquer un verrouillage de suppression pour Ã©viter la suppression accidentelle de la base de donnÃ©es.
- Gardez une trace des bases de donnÃ©es supprimÃ©es, nettoyez les instantanÃ©s antÃ©rieurs avec une seule commande.
- Utiliser cli pour gÃ©rer les bases de donnÃ©es comme kubectl pour Kubernetes.
> CRD: https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/

### Installation

L'[installation](https://kubedb.com/) est plÃ»tot simple.

```bash
$ curl -fsSL https://github.com/kubedb/installer/raw/v0.13.0-rc.0/deploy/kubedb.sh | bash
```

AprÃ¨s avoir fais la commande ci-dessus, on peut tester si KubeDB est installÃ© avec un `kubedb`.
> Si la commande `kubedb` retourne une Ã©rreur. La commande `curl -fsSL https://raw.githubusercontent.com/kubedb/installer/89fab34cf2f5d9e0bcc3c2d5b0f0599f94ff0dca/deploy/kubedb.sh | bash` peut possiblement installer correctement kubedb. 
> 
> [**Post orginal**](https://github.com/kubedb/issues/issues/691)

### Utilisation

Dans notre cas, KubeDB va nous servir Ã  crÃ©er le service mysql.
Nous allons aussi mettre en place un service PhpMyAdmin pour pouvoir monitorer la base de donnÃ©e.

#### La base de donnÃ©e

Pour crÃ©er cette base de donnÃ©e, il nous faut un fichier de config:

```yaml
mysql.yaml

apiVersion: kubedb.com/v1alpha1
kind: MySQL
metadata:
  name: picroma-mysql
  namespace: picroma-namespace
spec:
  databaseSecret:
    secretName: mysql-auth
    username: root
    password: password
  version: "8.0.14"
  doNotPause: true
  storage:
    storageClassName: "standard"
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 50Mi
```

Pour utiliser ce fichier de configuration avec KubeDB, on fait:

```bash
kubedb create -f ${fichier.yaml}
```

On vÃ©rifie:

```bash
$ kubedb get my
NAME            VERSION   STATUS    AGE
picroma-mysql   8.0.14    Running   3h29m
```

Nous avons notre base de donnÃ©e !
Passons donc Ã  PhpMyAdmin.

#### PhpMyAdmin

Pour avoir un PhpMyAdmin entiÃ¨rement fonctionnel, il nous faut un pod contenant l'app et un LoadBalancer

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myadmin
  name: myadmin
  namespace: picroma-namespace
spec:
  selector:
    matchLabels:
      app: myadmin
  replicas: 1
  template:
    metadata:
      labels:
        app: myadmin
    spec:
      containers:
      - image: phpmyadmin/phpmyadmin
        imagePullPolicy: Always
        name: phpmyadmin
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        env:
          - name: PMA_ARBITRARY
            value: '1'

---

apiVersion: v1
kind: Service
metadata:
  labels:
    app: myadmin
  name: myadmin
  namespace: picroma-namespace
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: http
  selector:
    app: myadmin
  type: LoadBalancer
```

Il nous suffit de `kubectl create -f ${file.yaml}`.

On check !

```bash
ewillian@ewillian:~/Documents/KubeWorld/PicromaConfig/Deployment$ kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
myadmin-58cd6c758c-6qz9c   1/1     Running   0          95m
picroma-mysql-0            1/1     Running   0          3h56
```

```bash
NAME                TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
myadmin             LoadBalancer   10.109.86.98   <pending>     80:30178/TCP   105m
picroma-mysql       ClusterIP      10.98.63.227   <none>        3306/TCP       3h57m
picroma-mysql-gvr   ClusterIP      None           <none>        3306/TCP       3h57m
```

Tout est en ordre.

Maintenant il nous faut nous connecter au service mysql via PhpMyAdmin.
Pour cela on rÃ©cupÃ¨re l'ip sur service PhpMyAdmin:

```bash
$ minikube service myadmin -n picroma-namespace --url
http://10.0.2.15:30178
```

On met cette ip dans notre navigateur.

![](https://i.imgur.com/bH0UHpb.png)

Ã‡a fonctionne !

Maintenant il nous faut les identifiant.

Dans le fichier de config du service mysql, nous avons indiquer Ã  mysql de garder des identifiant dans un `secret`.

```
spec:
  databaseSecret:
    secretName: mysql-auth
    username: root
    password: password
```

Pour vÃ©rifier / rÃ©cupÃ©rer ces identifiant, il nous faut effectuer:

```bash
$   kubectl get secrets -n picroma-namespace picroma-mysql-auth -o jsonpath='{.data.\username}' | base64 -d

root

$   kubectl get secrets -n picroma-namespace picroma-mysql-auth -o jsonpath='{.data.\password}' | base64 -d

XUr3vbwW-2p-wJsa
```

Enfin, l'ip du pod mysql:

```bash
$ kubectl get pods picroma-mysql-0 -n picroma-namespace -o yaml | grep podIP
  
  podIP: 172.17.0.6
```

Plus qu'Ã  se connecter !

![](https://i.imgur.com/EspEBGr.png)

## ğŸŒ Wordpress ğŸŒ

Dans cette partie, nous allons refaire l'infrastructure prÃ©cÃ©dente avec wordpress.

### Composants

Pour cela, nous vons besoin d'un namespace.

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: rockstar-namespace
  labels:
    name: rockstar-namespace
```

Du dÃ©ploiement mysql kubedb lÃ©gÃ¨rement modifiÃ©.

```yaml
apiVersion: kubedb.com/v1alpha1
kind: MySQL
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
  namespace: rockstar-namespace
spec:
  version: "5.7.25"
  storage:
    storageClassName: "standard"
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
init:
  scriptSource:
    local:
      path: ./scripts/init-wordpress-database.sql
```

D'un Volume Persistent.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
```

D'un objet Secret pour l'utilisateur wordpress (chiffrÃ© en Base64).

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: d29yZHByZXNz
  password: d29yZHByZXNz
```

Enfin, d'un script SQL pour initialiser l'utilisateur wordpress.

```sql
CREATE DATABASE IF NOT EXISTS wordpress;
CREATE USER 'wordpress'@'%' IDENTIFIED BY 'wordpress';
GRANT ALL PRIVILEGES ON wordpress.* TO 'wordpress'@'%';
USE wordpress;
```

### Ã‰tape de mise en place

- (1) CrÃ©ation du Namespace.

```
kubectl apply -f namespace.yaml
```

**Resultat :**

- (2) CrÃ©ation du Secret.

```
kubectl apply -f wordpress-auth-mysql.yaml
```

**Resultat :**

- (3) CrÃ©ation Du volume persistent.

```
kubectl apply -f mysql-deployment.yaml
```

**Resultat :**

- (4) Application du dÃ©ploiement KubeDB Mysql.

```
kubedb create -f mysql.yaml
```

**Resultat :**

- (5) Application du dÃ©ploiement Wordpress

```
kubectl apply -f wordpress-deployment.yaml
```

**Resultat :**

- (6) RÃ©cupÃ©ration URL et Connexion

```
sudo minikube service wordpress -n rockstar-namespace --url
```

**Resultat :**

## ğŸ§” RBAC (Role-Based Access Control) ğŸ§”

Le contrÃ´le dâ€™accÃ¨s basÃ© sur les rÃ´les (RBAC) est une mÃ©thode de rÃ©gulation de lâ€™accÃ¨s aux ordinateurs et aux ressources rÃ©seau basÃ©e sur les rÃ´les des utilisateurs individuels au sein dâ€™une entreprise. Nous pouvons utiliser le contrÃ´le dâ€™accÃ¨s basÃ© sur les rÃ´les sur toutes les ressources Kubernetes supportant les accÃ¨s CRUD (Create, Read, Update, Delete). 

### CrÃ©ation d'utilisateur

Les utilisateurs normaux sont supposÃ©s Ãªtre gÃ©rÃ©s par un service externe indÃ©pendant. Un administrateur distribuant des clÃ©s privÃ©es, un magasin dâ€™utilisateurs comme Keystone ou des comptes Google, voire un fichier contenant une liste de noms dâ€™utilisateur et de mots de passe. Ã€ cet Ã©gard, Kubernetes nâ€™a pas dâ€™objets qui reprÃ©sentent des comptes dâ€™utilisateur normaux. Les utilisateurs normaux ne peuvent pas Ãªtre ajoutÃ©s Ã  un cluster via un appel dâ€™API.

Dans notre cas, nous utiliserons les certificats clients X.509 avec OpenSSL pour leur simplicitÃ©. Il existe diffÃ©rentes Ã©tapes pour la crÃ©ation de ces utilisateurs.

- CrÃ©ation dâ€™un utilisateur sur la machine principale puis se rendre dans sa home pour effectuer les Ã©tapes restantes.

```
adduser geraldderive

cd /home/geraldderive
```

**Resultat:**

```
ewillian@ewillian:/home$ sudo adduser geraldderive

Ajout de l utilisateur Â«Â geraldderiveÂ Â» ...
Ajout du nouveau groupe Â«Â geraldderiveÂ Â» (1002) ...
Ajout du nouvel utilisateur Â«Â geraldderiveÂ Â» (1002) avec le groupe Â«Â geraldderiveÂ Â» ...
CrÃ©ation du rÃ©pertoire personnel Â«Â /home/geraldderiveÂ Â»...
Copie des fichiers depuis Â«Â /etc/skelÂ Â»...
Entrez le nouveau mot de passe UNIX : 
Retapez le nouveau mot de passe UNIX : 
passwdÂ : le mot de passe a Ã©tÃ© mis Ã  jour avec succÃ¨s
Modification des informations relatives Ã  l utilisateur geraldderive
Entrez la nouvelle valeur ou Â«Â EntrÃ©eÂ Â» pour conserver la valeur proposÃ©e
    Nom complet []: Gerald De Rive
    NÂ° de bureau []: 
    TÃ©lÃ©phone professionnel []: 
    TÃ©lÃ©phone personnel []: 
    Autre []: 
Ces informations sont-elles correctesÂ ? [O/n] o
```


- CrÃ©ation de sa private key :

```
openssl genrsa -out geraldderive.key 2048
```

- CrÃ©ation dâ€™une demande de signature de certificat (CSR). CN est le nom de lâ€™utilisateur et O est le groupe. Il est possible de dÃ©finir des autorisations Ã  lâ€™Ã©chelle dâ€™un groupe, ce qui peut simplifier la gestion si plusieurs utilisateurs partagent les mÃªmes autorisations.

```
# Without Group
openssl req -new \
-key geraldderive.key \
-out geraldderive.csr \
-subj "/CN=geraldderive"

# With a Group where $group is the group name
openssl req -new \
-key geraldderive.key \
-out geraldderive.csr \
-subj "/CN=geraldderive/O=$group"

#If the user has multiple groups
openssl req -new \
-key geraldderive.key \
-out geraldderive.csr \
-subj "/CN=geraldderive/O=$group1/O=$group2/O=$group3"
```

- Signer le CSR avec le CA de Kubernetes. Le certificat et la clÃ© de Kubernetes sont locallisÃ©s dans /etc/kubernetes/pki. Les certificats gÃ©nÃ©rÃ©s ci-dessous seront valides pour 500 jours.

```
openssl x509 -req \
-in geraldderive.csr \
-CA /etc/kubernetes/pki/ca.crt \
-CAkey /etc/kubernetes/pki/ca.key \
-CAcreateserial \
-out geraldderive.crt -days 500
```

- CrÃ©ation dâ€™un rÃ©pertoire â€œ.certsâ€ oÃ¹ sera stockÃ© les clÃ© public et privÃ©es de lâ€™utilisateur.

```
mkdir .certs && mv geraldderive.crt geraldderive.key .certs
```

- CrÃ©ation de lâ€™utilisateur dans Kubernetes.

```
kubectl config set-credentials geraldderive \
--client-certificate=/home/geraldderive/.certs/geraldderive.crt \
--client-key=/home/geraldderive/.certs/geraldderive.key
```

- CrÃ©ation dâ€™un contexte associÃ© Ã  lâ€™utilisateur.

```
kubectl config set-context geraldderive-context \
--cluster=kubernetes --user=geraldderive
```

- Edition du fichier de configuration utilisateur. Ce fichier de configuration contient toutes les informations nÃ©cessaire pour authentifier lâ€™utilisateur auprÃ¨s du cluster. Vous pouvez utiliser la configuration de lâ€™administrateur du cluster comme template. Il se trouve normalement dans /etc/kubernetes/. Les variables â€œcertificate-authority-dataâ€ et â€œserverâ€ doivent Ãªtre identiques Ã  celle de lâ€™administrateur.

```
apiVersion: v1
clusters:
- cluster:
 certificate-authority-data: {Parse content here}
 server: {Parse content here}
name: kubernetes
contexts:
- context:
 cluster: kubernetes
 user: geraldderive
name: geraldderive-context
current-context: geraldderive-context
kind: Config
preferences: {}
users:
- name: geraldderive
user:
 client-certificate: /home/geraldderive/.certs/geraldderive.cert
 client-key: /home/geraldderive/.certs/geraldderive.key
```

- Ensuite, nous devons copier la configuration ci-dessus dans le rÃ©pertoire .kube.

```
mkdir .kube && vi .kube/config
```

- Appliquer les permission sur tous les fichiers et rÃ©pertoires associÃ©s Ã  lâ€™utilisateur :

```
chown -R geraldderive: /home/geraldderive/
```

- CrÃ©ation du namespace et on vÃ©rifie si l'utilisateur peut Ã©ffeectuer les commandes qui lui sont interdites.



****
**Optionnel**
****

## ğŸ–¥ï¸ Monitoring ğŸ–¥ï¸



## ğŸ” OIDC (OpenID Connect) ğŸ”



## ğŸ§§ Registry + GUI Web (Graphical User Interface Web) ğŸ§§

## Annexes

### Membres dux projet

- Guillaume LE COQ
- Benoit GALMOT
- Souleimane SEGHIR

### Plugins Kubernetes utilisÃ©s

#### Kubens / Kubectx

https://github.com/ahmetb/kubectx

```
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens```